import asyncio
import argparse
from telethon.sync import TelegramClient
from telethon.errors import (
    FloodWaitError, ChannelPrivateError, ChatWriteForbiddenError, RPCError,
    SessionPasswordNeededError, PhoneNumberInvalidError, ApiIdInvalidError
)
import re
import sqlite3
import os
import time
from urllib.parse import unquote
from datetime import datetime, timezone
import logging
import sys

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('telegram_parser.log', mode='a+', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# åˆ‡æ¢åˆ°è„šæœ¬æ‰€åœ¨ç›®å½•
os.chdir(os.path.dirname(os.path.abspath(__file__)))
logger.debug(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")

# å‘½ä»¤è¡Œå‚æ•°
parser = argparse.ArgumentParser(description='Telegramæ¶ˆæ¯æå–è„šæœ¬')
parser.add_argument('--reset', action='store_true', help='æ¸…ç©ºæ•°æ®åº“ï¼Œå¼ºåˆ¶å…¨æ–°å¤„ç†')
args = parser.parse_args()

# Telegramé…ç½®
api_id = '6627460'
api_hash = '27a53a0965e486a2bc1b1fcde473b1c4'
phone = ''
channels = ['@tgsearchers2', '@tianyirigeng', '@tyysypzypd', '@tyypzhpd', '@cloudtianyi', '@kuakeclound']

# åˆå§‹åŒ–æ•°æ®åº“
db_path = os.getenv('DB_PATH', 'data.db')
conn = sqlite3.connect(db_path, check_same_thread=False)
conn.execute('PRAGMA journal_mode=WAL')
conn.execute('PRAGMA synchronous=NORMAL')
conn.execute('PRAGMA temp_store=MEMORY')
cursor = conn.cursor()

# æ¸…ç©ºæ•°æ®åº“ï¼ˆ--resetï¼‰
if args.reset:
    logger.info("æ¸…ç©ºæ•°æ®åº“ï¼Œå‡†å¤‡å…¨æ–°å¤„ç†")
    try:
        conn.execute('BEGIN TRANSACTION')
        cursor.execute('DELETE FROM last_processed_messages')
        cursor.execute('DELETE FROM x_storages')
        conn.commit()
        logger.info("æ•°æ®åº“å·²é‡ç½®")
    except sqlite3.Error as e:
        conn.rollback()
        logger.error(f"æ¸…ç©ºæ•°æ®åº“å¤±è´¥: {str(e)}")
        raise


# åˆ›å»ºè¡¨å¹¶æ·»åŠ ç´¢å¼•
def init_database():
    """åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„"""
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS last_processed_messages (
            channel TEXT PRIMARY KEY,
            last_message_id INTEGER
        )
    ''')
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS x_storages (
            id INTEGER PRIMARY KEY,
            mount_path TEXT,
            "order" INTEGER,
            driver TEXT,
            cache_expiration INTEGER,
            status TEXT,
            addition TEXT,
            remark TEXT,
            disabled INTEGER,
            order_by TEXT,
            order_direction TEXT,
            web_proxy INTEGER,
            webdav_policy TEXT,
            down_proxy_url TEXT,
            disable_index INTEGER,
            enable_sign INTEGER,
            proxy_range INTEGER,
            modified TEXT,
            extract_folder TEXT
        )
    ''')
    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_x_storages_mount_path
        ON x_storages(mount_path)
    ''')
    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_x_storages_addition
        ON x_storages(addition)
    ''')
    conn.commit()


init_database()

# è·å–æœ€å¤§ID
cursor.execute('SELECT MAX(id) FROM x_storages')
max_id = cursor.fetchone()[0]
current_id = 30000 if max_id is None or max_id < 30000 else max_id + 1
logger.info(f"å½“å‰èµ·å§‹ID: {current_id}")

# ä¸»æ­£åˆ™è¡¨è¾¾å¼
MASTER_PATTERN = re.compile(
    r'(?P<title>åç§°ï¼š\s*(.*?))(?=\n*(?:https?://|\[ã€\(|\]ã€‘\)|\[|ğŸ·|_æ ‡ç­¾ï¼š|$))|'
    r'(?P<url>https?://cloud\.189\.cn(?:/web)?/(?:share\.html\?shareId=|t/|web/shareDetail\.do\?sid=|s/)(?P<share_id>[a-zA-Z0-9\-_]+))|'
    r'(?:è®¿é—®ç |æå–ç |å¯†ç |éªŒè¯ç )[ï¼š:\s]*(?P<code>[a-zA-Z0-9]{4})',  # è°ƒæ•´ä¸ºä»…æ•è·4ä½ç 
    re.DOTALL | re.MULTILINE | re.IGNORECASE
)


# æ›¿æ¢ç©ºæ ¼
def replace_spaces(title):
    """å°†æ ‡é¢˜ä¸­çš„ç©ºæ ¼æ›¿æ¢ä¸ºä¸‹åˆ’çº¿"""
    return title.replace(' ', '_')


# è·å–å”¯ä¸€ mount_path
def get_unique_mount_path(base_path, used_paths):
    """åŸºäºå·²ä½¿ç”¨è·¯å¾„ç”Ÿæˆå”¯ä¸€ mount_path"""
    if base_path not in used_paths:
        used_paths.add(base_path)
        return base_path
    counter = 1
    while True:
        new_path = f"{base_path}_{counter}"
        if new_path not in used_paths:
            used_paths.add(new_path)
            return new_path
        counter += 1


# è·å–æ—¥æœŸæ–‡ä»¶å
def get_date_filename():
    """ç”Ÿæˆæ—¥æœŸæ–‡ä»¶åï¼Œå¦‚ 189share-06-10.txt"""
    return f"189share-{datetime.now(timezone.utc).strftime('%m-%d')}.txt"


# æ‰¹é‡å†™å…¥æ–‡ä»¶
def batch_save_to_text(records):
    """æ‰¹é‡ä¿å­˜è®°å½•åˆ°æ–‡ä»¶ï¼Œé˜²æ­¢é‡å¤"""
    if not records:
        return

    date_file = f"{get_date_filename()}"
    # ç§»é™¤ä¸å¿…è¦çš„ç›®å½•åˆ›å»ºï¼Œå› ä¸ºæ–‡ä»¶ç›´æ¥ä¿å­˜åˆ°å½“å‰ç›®å½•
    try:
        with open(date_file, 'a', encoding='utf-8') as f:
            for mount_path, share_id, share_pwd in records:
                f.write(f"{mount_path} 9:{share_id} root {share_pwd}\n")
        logger.info(f"æ‰¹é‡å†™å…¥ {len(records)} æ¡è®°å½•åˆ° {date_file}")
    except Exception as e:
        logger.error(f"å†™å…¥æ–‡ä»¶å¤±è´¥: {str(e)}")


# åˆ†ç±»å‡½æ•°
def should_put_in_active_folder(message_text, title):
    """
    ä»…æ ¹æ® active_keywords åˆ¤æ–­æ´»è·ƒèµ„æºï¼Œé active_keywords å½’ç±»ä¸ºå¤©ç¿¼å®Œç»“
    """
    combined = f"{title} {message_text}".lower()
    active_keywords = ["æ›´æ–°ä¸­", "æ›´æ–°è‡³", "æ›´è‡³", "é¦–æ›´", "è¿è½½ä¸­", "æŒç»­æ›´æ–°", "å‘¨æ›´", "æ—¥æ›´"]

    for kw in active_keywords:
        if kw in combined:
            logger.debug(f"åˆ¤æ–­ä¸ºæ´»è·ƒèµ„æºï¼Œä¾æ®: {kw}")
            return True

    logger.debug("åˆ¤æ–­ä¸ºå®Œç»“èµ„æºï¼Œæ— æ´»è·ƒæ ‡è®°")
    return False


# æå–æ ‡é¢˜
def extract_title(text):
    """ä»æ¶ˆæ¯æ–‡æœ¬ä¸­æå–æ ‡é¢˜"""
    for match in MASTER_PATTERN.finditer(text):
        if match.group('title'):
            title = match.group('title').strip()
            if title:
                return unquote(title)

    for line in text.split('\n'):
        line = line.strip()
        if line and not line.startswith(('http', 'https', 'ğŸ·', '_æ ‡ç­¾ï¼š', 'ç®€ä»‹ï¼š', 'åˆ†äº«ï¼š')):
            return unquote(line)

    url_match = MASTER_PATTERN.search(text)
    if url_match and url_match.group('url'):
        return f"æœªå‘½åèµ„æº_{url_match.group('share_id')}"

    return f"æœªå‘½åèµ„æº_{int(time.time())}"


# æå–é“¾æ¥å’Œè®¿é—®ç 
def extract_links(text):
    """æå–æ‰€æœ‰å¤©ç¿¼é“¾æ¥å’Œå¯¹åº”çš„è®¿é—®ç """
    links = []
    last_url_index = -1

    for match in MASTER_PATTERN.finditer(text):
        if match.group('url'):
            share_id = match.group('share_id')
            # ä½¿ç”¨å‰ä¸€ä¸ªè®¿é—®ç ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            code = links[last_url_index][1] if last_url_index >= 0 else ""
            links.append((share_id, code))
            last_url_index += 1
        elif match.group('code'):
            code = match.group('code')  # ç›´æ¥ä½¿ç”¨æ•è·çš„4ä½ç 
            if last_url_index >= 0:
                links[last_url_index] = (links[last_url_index][0], code)  # æ›´æ–°å‰ä¸€ä¸ªé“¾æ¥çš„è®¿é—®ç 

    return links


# å¤„ç†å•ä¸ªæ¶ˆæ¯
def process_message(message, cursor, processed_share_codes, existing_records, current_id):
    """å¤„ç†å•æ¡æ¶ˆæ¯ï¼Œè¿”å›æ–°è®°å½•ã€æ–‡æœ¬è®°å½•å’Œå¾…åˆ é™¤ID"""
    if not message.text or not message.text.strip():
        logger.debug(f"æ¶ˆæ¯ ID: {message.id} æ— æ–‡æœ¬ï¼Œè·³è¿‡")
        return [], [], []

    try:
        raw_title = extract_title(message.text)
        clean_title = raw_title.replace('*', '').strip()
        if clean_title.startswith('åç§°ï¼š'):
            clean_title = clean_title[3:].strip()
        clean_title = re.sub(r'[)\]\}]+$', '', clean_title).strip()
        clean_title = replace_spaces(clean_title)
        # ç§»é™¤å¤šç§å¤šä½™æ–œæ å’Œè¿ç»­ä¸‹åˆ’çº¿
        clean_title = re.sub(r'__+|/[_/]*', '_', clean_title)
        # å»é™¤é¦–å°¾å¤šä½™çš„ _ æˆ– /
        clean_title = clean_title.strip('_/ ')
        logger.debug(f"æå–æ ‡é¢˜: {clean_title}")

        links = extract_links(message.text)
        if not links:
            logger.debug(f"æ¶ˆæ¯ ID: {message.id} æ— å¤©ç¿¼é“¾æ¥ï¼Œè·³è¿‡")
            return [], [], []

        folder = "å¤©ç¿¼åˆ†äº«" if should_put_in_active_folder(message.text, clean_title) else "å¤©ç¿¼å®Œç»“"
        base_mount_path = f"/{folder}/{clean_title}"

        new_records = []
        text_records = []
        delete_ids = set()
        used_paths = {}  # å­˜å‚¨åˆ†äº«ç å¯¹åº”çš„ç¬¬ä¸€ä¸ª mount_path

        for i, (share_id, share_code) in enumerate(links):
            # è·³è¿‡å·²å¤„ç†çš„åˆ†äº«ç ï¼ˆè·¨é¢‘é“å»é‡ï¼‰
            if share_id in processed_share_codes:
                logger.debug(f"è·³è¿‡å·²å¤„ç†åˆ†äº«ç : {share_id}")
                continue

            processed_share_codes.add(share_id)

            # ä½¿ç”¨ç¬¬ä¸€ä¸ªå‡ºç°çš„ mount_path
            if share_id not in used_paths:
                mount_path = get_unique_mount_path(base_mount_path, set(used_paths.values()))
                used_paths[share_id] = mount_path
            else:
                mount_path = used_paths[share_id]

            logger.debug(f"Generated mount_path for {share_id}: {mount_path}")

            record_id = current_id + len(new_records)
            record = (
                record_id,
                '/æˆ‘çš„å¤©ç¿¼åˆ†äº«' + mount_path,
                0,
                '189Share',
                30,
                'work',
                f'{{"share_id":"{share_id}","share_pwd":"{share_code}","ShareToken":"","root_folder_id":""}}',
                '',
                0,
                'name',
                'ASC',
                0,
                '302_redirect',
                '',
                0,
                0,
                0,
                datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S+00:00'),
                ''
            )
            new_records.append(record)
            text_records.append((mount_path, share_id, share_code))
            logger.info(
                f"å‡†å¤‡æ–°è®°å½•: ID={record_id}, åˆ†äº«ç : {share_id}, è®¿é—®ç : {share_code}, mount_path: {mount_path}")

            # å¦‚æœåˆ†äº«ç å·²å­˜åœ¨æ•°æ®åº“ï¼Œæ ‡è®°åˆ é™¤æ—§è®°å½•
            if share_id in existing_records and share_id not in processed_share_codes:  # ä»…åœ¨æ–°æ·»åŠ æ—¶åˆ é™¤
                delete_ids.add(existing_records[share_id])
                del existing_records[share_id]
            existing_records[share_id] = record_id

        return new_records, text_records, list(delete_ids)

    except Exception as e:
        logger.error(f"å¤„ç†æ¶ˆæ¯ ID: {message.id} å¤±è´¥: {str(e)}")
        return [], [], []


# æ‰¹é‡æ’å…¥è®°å½•
def batch_insert_records(records):
    """æ‰¹é‡æ’å…¥è®°å½•åˆ°æ•°æ®åº“"""
    if not records:
        return 0

    try:
        conn.execute('BEGIN TRANSACTION')
        cursor.executemany('''
            INSERT OR IGNORE INTO x_storages (
                id, mount_path, "order", driver, cache_expiration,
                status, addition, remark, disabled, order_by,
                order_direction, web_proxy, webdav_policy, down_proxy_url,
                disable_index, enable_sign, proxy_range, modified, extract_folder
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', records)
        conn.commit()
        inserted = cursor.rowcount
        logger.info(f"æˆåŠŸæ’å…¥ {inserted} æ¡è®°å½•")
        return inserted
    except sqlite3.IntegrityError as e:
        conn.rollback()
        logger.warning(f"æ‰¹é‡æ’å…¥å¤±è´¥: {str(e)}ï¼Œå°è¯•é€æ¡æ’å…¥")
        return insert_one_by_one(records)
    except sqlite3.Error as e:
        conn.rollback()
        logger.error(f"æ•°æ®åº“æ“ä½œå¤±è´¥: {str(e)}")
        return 0


def insert_one_by_one(records):
    """é€æ¡æ’å…¥è®°å½•"""
    inserted = 0
    for record in records:
        try:
            cursor.execute('''
                INSERT OR IGNORE INTO x_storages (
                    id, mount_path, "order", driver, cache_expiration,
                    status, addition, remark, disabled, order_by,
                    order_direction, web_proxy, webdav_policy, down_proxy_url,
                    disable_index, enable_sign, proxy_range, modified, extract_folder
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', record)
            conn.commit()
            inserted += 1
        except sqlite3.Error as e:
            conn.rollback()
            logger.error(f"æ’å…¥è®°å½•å¤±è´¥: {str(e)}")
    return inserted


# Telegramå®¢æˆ·ç«¯
client = TelegramClient('session', api_id, api_hash)


async def main():
    global current_id
    try:
        await client.start(phone)
        logger.info("Telegramå®¢æˆ·ç«¯å¯åŠ¨æˆåŠŸ")
    except (SessionPasswordNeededError, PhoneNumberInvalidError, ApiIdInvalidError) as e:
        logger.error(f"å®¢æˆ·ç«¯å¯åŠ¨å¤±è´¥: {str(e)}")
        sys.exit(1)

    processed_share_codes = set()
    existing_records = {}
    start_time = time.time()

    # åŠ è½½ç°æœ‰æ•°æ®åº“è®°å½•
    if not args.reset:
        try:
            cursor.execute('''
                SELECT id, json_extract(addition, '$.share_id')
                FROM x_storages
                WHERE driver = '189Share'
            ''')
            for row in cursor.fetchall():
                share_id = row[1]
                if share_id:
                    processed_share_codes.add(share_id)
                    existing_records[share_id] = row[0]
            logger.info(f"åŠ è½½ {len(existing_records)} æ¡è®°å½•ï¼Œè€—æ—¶: {time.time() - start_time:.2f}ç§’")
        except sqlite3.Error as e:
            logger.error(f"åŠ è½½è®°å½•å¤±è´¥: {str(e)}")

    for channel in channels:
        logger.info(f"å¼€å§‹å¤„ç†é¢‘é“: {channel}")

        cursor.execute('SELECT last_message_id FROM last_processed_messages WHERE channel = ?', (channel,))
        result = cursor.fetchone()
        last_message_id = result[0] if result else None

        min_id = last_message_id + 1 if last_message_id else 0
        message_limit = 100000
        reverse = False

        try:
            total_messages = await client.get_messages(channel, limit=1)
            if not total_messages or (last_message_id and total_messages[0].id <= last_message_id and not args.reset):
                logger.info(f"é¢‘é“ {channel} æ— æ–°æ¶ˆæ¯ï¼Œè·³è¿‡å¤„ç†")
                continue

            processed_count = 0
            max_id_in_batch = 0
            to_delete_ids = set()
            new_records_batch = []
            text_records_batch = []
            batch_start = time.time()

            async for message in client.iter_messages(
                    channel,
                    min_id=min_id,
                    limit=message_limit,
                    reverse=reverse,
                    wait_time=0.1  # å‡å°‘ç­‰å¾…æ—¶é—´
            ):
                if max_id_in_batch == 0:
                    max_id_in_batch = message.id

                new_records, text_records, delete_ids = process_message(
                    message, cursor, processed_share_codes, existing_records, current_id
                )
                to_delete_ids.update(delete_ids)

                if new_records:
                    new_records_batch.extend(new_records)
                    text_records_batch.extend(text_records)
                    current_id += len(new_records)
                    processed_count += len(new_records)

                # æ¯100æ¡æ£€æŸ¥å¤„ç†é€Ÿåº¦
                if processed_count % 100 == 0:
                    elapsed = time.time() - batch_start
                    if elapsed < 0.5:  # æ¯ç§’è¶…è¿‡200æ¡
                        sleep_time = min(0.5, (0.5 - elapsed) * 2)
                        logger.debug(f"å¤„ç†é€Ÿåº¦è¿‡å¿«ï¼Œä¼‘çœ  {sleep_time:.2f} ç§’")
                        await asyncio.sleep(sleep_time)

                # æ¯500æ¡æ‰¹é‡å¤„ç†
                if processed_count >= 500:
                    batch_insert_records(new_records_batch)
                    batch_save_to_text(text_records_batch)
                    new_records_batch.clear()
                    text_records_batch.clear()
                    processed_count = 0
                    batch_start = time.time()

            # å¤„ç†å‰©ä½™è®°å½•
            if new_records_batch:
                batch_insert_records(new_records_batch)
                batch_save_to_text(text_records_batch)
                logger.info(f"å¤„ç†å‰©ä½™ {len(new_records_batch)} æ¡è®°å½•")

            # æ‰¹é‡åˆ é™¤ä¼˜åŒ–
            if to_delete_ids:
                id_chunks = [list(to_delete_ids)[i:i + 500] for i in range(0, len(to_delete_ids), 500)]
                for chunk in id_chunks:
                    placeholders = ','.join('?' for _ in chunk)
                    cursor.execute(f'DELETE FROM x_storages WHERE id IN ({placeholders})', chunk)
                conn.commit()
                logger.info(f"ä»æ•°æ®åº“åˆ é™¤ {len(to_delete_ids)} æ¡æ—§è®°å½•")

            if max_id_in_batch > 0:
                cursor.execute('''
                    INSERT OR REPLACE INTO last_processed_messages (channel, last_message_id)
                    VALUES (?, ?)
                ''', (channel, max_id_in_batch))
                conn.commit()
                logger.info(f"æ›´æ–°æ£€æŸ¥ç‚¹: {channel} æœ€åID={max_id_in_batch}")

            logger.info(
                f"é¢‘é“ {channel} å¤„ç†å®Œæˆï¼Œå¤„ç† {processed_count} æ¡è®°å½•ï¼Œæ€»è€—æ—¶: {time.time() - start_time:.2f}ç§’")

        except FloodWaitError as e:
            wait_time = e.seconds + 10
            logger.warning(f"è§¦å‘é£æ§ï¼Œç­‰å¾… {wait_time} ç§’")
            await asyncio.sleep(wait_time)
        except (ChannelPrivateError, ChatWriteForbiddenError) as e:
            logger.error(f"é¢‘é“è®¿é—®å¤±è´¥: {str(e)}")
        except RPCError as e:
            logger.warning(f"APIé”™è¯¯: {str(e)}")
            await asyncio.sleep(5)
        except Exception as e:
            logger.error(f"å¤„ç†é¢‘é“ {channel} å¤±è´¥: {str(e)}", exc_info=True)


if __name__ == '__main__':
    try:
        with client:
            client.loop.run_until_complete(main())
    except Exception as e:
        logger.error(f"è„šæœ¬è¿è¡Œé”™è¯¯: {str(e)}", exc_info=True)
    finally:
        logger.info("æ‰§è¡Œæ•°æ®åº“ä¼˜åŒ–...")
        try:
            cursor.execute("PRAGMA optimize")
            conn.commit()
        except Exception as e:
            logger.error(f"æ•°æ®åº“ä¼˜åŒ–å¤±è´¥: {str(e)}")
        finally:
            conn.close()
            logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")